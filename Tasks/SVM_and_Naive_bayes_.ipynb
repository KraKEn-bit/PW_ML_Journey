{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### 1) What is a Support Vector Machine (SVM)\n",
        "\n",
        "**Answer:**\n",
        "A Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression tasks. It finds the optimal hyperplane that best separates the data into classes by maximizing the margin between the closest points of the classes, called support vectors.\n",
        "\n",
        "---\n",
        "\n",
        "### 2) What is the difference between Hard Margin and Soft Margin SVM\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "* **Hard Margin SVM** assumes the data is perfectly linearly separable and doesn't allow any misclassifications.\n",
        "* **Soft Margin SVM** allows some data points to be misclassified by introducing a penalty for violations. It is more robust in real-world scenarios where data is noisy or overlapping.\n",
        "\n",
        "---\n",
        "\n",
        "### 3) What is the mathematical intuition behind SVM\n",
        "\n",
        "**Answer:**\n",
        "SVM aims to maximize the margin between two classes by solving the following optimization problem:\n",
        "\n",
        "$$\n",
        "\\min \\frac{1}{2} \\|w\\|^2 \\quad \\text{subject to } y_i(w \\cdot x_i + b) \\geq 1\n",
        "$$\n",
        "\n",
        "Here, $w$ is the weight vector, $b$ is the bias, and $y_i$ is the class label of point $x_i$. The goal is to find the hyperplane that maximizes the separation margin.\n",
        "\n",
        "---\n",
        "\n",
        "### 4) What is the role of Lagrange Multipliers in SVM\n",
        "\n",
        "**Answer:**\n",
        "Lagrange multipliers are used to convert the constrained optimization problem into its dual form, making it easier to solve and enabling the use of the **kernel trick**. Non-zero Lagrange multipliers identify the **support vectors**—critical points that define the decision boundary.\n",
        "\n",
        "---\n",
        "\n",
        "### 5) What are Support Vectors in SVM\n",
        "\n",
        "**Answer:**\n",
        "Support vectors are the data points that lie closest to the decision boundary (margin). These points are the most influential in determining the position and orientation of the separating hyperplane.\n",
        "\n",
        "---\n",
        "\n",
        "### 6) What is a Support Vector Classifier (SVC)\n",
        "\n",
        "**Answer:**\n",
        "A Support Vector Classifier (SVC) is the classification implementation of SVM. It classifies data into categories by finding the optimal separating hyperplane and supports various kernel functions for non-linear decision boundaries.\n",
        "\n",
        "---\n",
        "\n",
        "### 7) What is a Support Vector Regressor (SVR)\n",
        "\n",
        "**Answer:**\n",
        "Support Vector Regressor (SVR) is the regression counterpart of SVM. It fits a function within a margin of tolerance (ε), and penalizes data points that fall outside this margin, minimizing the prediction error.\n",
        "\n",
        "---\n",
        "\n",
        "### 8) What is the Kernel Trick in SVM\n",
        "\n",
        "**Answer:**\n",
        "The Kernel Trick allows SVM to perform non-linear classification by mapping input features into a higher-dimensional space without explicitly computing the transformation. It replaces dot products with kernel functions like linear, polynomial, or RBF to make computations efficient.\n",
        "\n",
        "---\n",
        "\n",
        "### 9) Compare Linear Kernel, Polynomial Kernel, and RBF Kernel\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "* **Linear Kernel**: Suitable for linearly separable data. Fast and interpretable.\n",
        "* **Polynomial Kernel**: Adds interaction terms, allowing curved boundaries. Degree parameter controls complexity.\n",
        "* **RBF (Radial Basis Function)**: Handles highly non-linear data well. Uses a Gaussian function to map points to infinite-dimensional space. Most commonly used.\n",
        "\n",
        "---\n",
        "\n",
        "### 10) What is the effect of the C parameter in SVM\n",
        "\n",
        "**Answer:**\n",
        "The **C** parameter controls the trade-off between maximizing the margin and minimizing classification error.\n",
        "\n",
        "* High **C**: Less regularization, tries to classify all training points correctly (may overfit).\n",
        "* Low **C**: More regularization, allows more margin violations (may underfit but generalizes better).\n",
        "\n",
        "---\n",
        "\n",
        "### 11) What is the role of the Gamma parameter in RBF Kernel SVM\n",
        "\n",
        "**Answer:**\n",
        "Gamma defines how far the influence of a single training example reaches:\n",
        "\n",
        "* High gamma: Short-range influence, tight decision boundaries (may overfit).\n",
        "* Low gamma: Long-range influence, smoother decision boundaries (may underfit).\n",
        "\n",
        "---\n",
        "\n",
        "### 12) What is the Naïve Bayes classifier, and why is it called \"Naïve\"\n",
        "\n",
        "**Answer:**\n",
        "Naïve Bayes is a probabilistic classifier based on Bayes’ Theorem. It is \"naïve\" because it assumes all features are **conditionally independent** given the class label—a strong and rarely true assumption, but often works well in practice.\n",
        "\n",
        "---\n",
        "\n",
        "### 13) What is Bayes’ Theorem\n",
        "\n",
        "**Answer:**\n",
        "Bayes’ Theorem is a fundamental formula to update probabilities based on new evidence:\n",
        "\n",
        "$$\n",
        "P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}\n",
        "$$\n",
        "\n",
        "It calculates the probability of event A happening given that event B has occurred.\n",
        "\n",
        "---\n",
        "\n",
        "### 14) Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "* **Gaussian Naïve Bayes**: For continuous data assuming a normal (Gaussian) distribution.\n",
        "* **Multinomial Naïve Bayes**: For count-based data (e.g., word counts in text).\n",
        "* **Bernoulli Naïve Bayes**: For binary/boolean features (e.g., word presence/absence).\n",
        "\n",
        "---\n",
        "\n",
        "### 15) When should you use Gaussian Naïve Bayes over other variants\n",
        "\n",
        "**Answer:**\n",
        "Use Gaussian Naïve Bayes when your features are **continuous** and **normally distributed**. Common in numerical datasets like sensor data or medical measurements.\n",
        "\n",
        "---\n",
        "\n",
        "### 16) What are the key assumptions made by Naïve Bayes\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "* All features are **conditionally independent** given the class label.\n",
        "* Each feature contributes **equally and independently** to the outcome.\n",
        "  These assumptions simplify computation but are rarely true in practice.\n",
        "\n",
        "---\n",
        "\n",
        "### 17) What are the advantages and disadvantages of Naïve Bayes\n",
        "\n",
        "**Advantages:**\n",
        "\n",
        "* Simple and fast to train\n",
        "* Works well with high-dimensional data\n",
        "* Performs well on text classification problems\n",
        "\n",
        "**Disadvantages:**\n",
        "\n",
        "* Assumes feature independence\n",
        "* May perform poorly when features are highly correlated\n",
        "\n",
        "---\n",
        "\n",
        "### 18) Why is Naïve Bayes a good choice for text classification\n",
        "\n",
        "**Answer:**\n",
        "Naïve Bayes works well for text because:\n",
        "\n",
        "* Text data is high-dimensional, which NB handles efficiently.\n",
        "* Bag-of-words representations fit the independence assumption reasonably well.\n",
        "* Fast training and prediction even on large datasets.\n",
        "\n",
        "---\n",
        "\n",
        "### 19) Compare SVM and Naïve Bayes for classification tasks\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "| Feature         | SVM                          | Naïve Bayes                  |\n",
        "| --------------- | ---------------------------- | ---------------------------- |\n",
        "| Type            | Discriminative               | Generative                   |\n",
        "| Accuracy        | Often higher                 | Competitive in text          |\n",
        "| Training Speed  | Slower                       | Much faster                  |\n",
        "| Feature Scaling | Required                     | Not required                 |\n",
        "| Assumption      | No distributional assumption | Conditional independence     |\n",
        "| Best Use Case   | Complex boundaries           | High-dimensional sparse data |\n",
        "\n",
        "---\n",
        "\n",
        "### 20) How does Laplace Smoothing help in Naïve Bayes?\n",
        "\n",
        "**Answer:**\n",
        "Laplace Smoothing (also called additive smoothing) prevents zero probabilities for words or features not seen in training data. It adds 1 (or a small value) to all feature counts, ensuring that every possible feature has a non-zero probability.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xPi6J2bFpgIE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNYPMP3NpdXT"
      },
      "outputs": [],
      "source": [
        "#1) Write a Python program to train an SVM Classifier on the Iris dataset and evaluate accuracy:\n",
        "from sklearn.datasets import load_iris\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "clf = SVC()\n",
        "clf.fit(X_train, y_train)\n",
        "print(\"Accuracy (Iris SVM):\", clf.score(X_test, y_test))\n",
        "\n",
        "#2) Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then compare their accuracies:\n",
        "from sklearn.datasets import load_wine\n",
        "X, y = load_wine(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "svc_linear = SVC(kernel='linear').fit(X_train, y_train)\n",
        "svc_rbf = SVC(kernel='rbf').fit(X_train, y_train)\n",
        "print(\"Linear Kernel Accuracy:\", svc_linear.score(X_test, y_test))\n",
        "print(\"RBF Kernel Accuracy:\", svc_rbf.score(X_test, y_test))\n",
        "\n",
        "#3) Write a Python program to train an SVM Regressor (SVR) on a housing dataset and evaluate it using Mean Squared Error (MSE):\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "X, y = fetch_california_housing(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "svr = SVR()\n",
        "svr.fit(X_train, y_train)\n",
        "y_pred = svr.predict(X_test)\n",
        "print(\"SVR MSE:\", mean_squared_error(y_test, y_pred))\n",
        "\n",
        "#4) Write a Python program to train an SVM Classifier with a Polynomial Kernel and visualize the decision boundary:\n",
        "from sklearn.datasets import make_classification\n",
        "X, y = make_classification(n_features=2, n_redundant=0, n_informative=2, n_clusters_per_class=1)\n",
        "clf = SVC(kernel='poly', degree=3).fit(X, y)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm')\n",
        "plt.title(\"SVM with Polynomial Kernel\")\n",
        "plt.show()\n",
        "\n",
        "#5) Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer dataset and evaluate accuracy:\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "print(\"Accuracy (Gaussian NB):\", gnb.score(X_test, y_test))\n",
        "\n",
        "#6) Write a Python program to train a Multinomial Naïve Bayes classifier for text classification using the 20 Newsgroups dataset:\n",
        "from sklearn.datasets import fetch_20newsgroups_vectorized\n",
        "data = fetch_20newsgroups_vectorized()\n",
        "X, y = data.data, data.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "mnb = MultinomialNB()\n",
        "mnb.fit(X_train, y_train)\n",
        "print(\"Accuracy (Multinomial NB):\", mnb.score(X_test, y_test))\n",
        "\n",
        "#7) Write a Python program to train an SVM Classifier with different C values and compare the decision boundaries visually:\n",
        "X, y = make_classification(n_features=2, n_redundant=0, n_informative=2, n_clusters_per_class=1)\n",
        "for c in [0.1, 1, 10]:\n",
        "    model = SVC(C=c).fit(X, y)\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='bwr', alpha=0.6)\n",
        "    plt.title(f'SVM Decision Boundary with C={c}')\n",
        "    plt.show()\n",
        "\n",
        "#8) Write a Python program to train a Bernoulli Naïve Bayes classifier for binary classification on a dataset with binary features:\n",
        "X = np.random.randint(0, 2, size=(100, 10))\n",
        "y = np.random.randint(0, 2, size=(100,))\n",
        "bnb = BernoulliNB()\n",
        "bnb.fit(X, y)\n",
        "print(\"Accuracy (Bernoulli NB):\", bnb.score(X, y))\n",
        "\n",
        "#9) Write a Python program to apply feature scaling before training an SVM model and compare results with unscaled data:\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "svc = SVC()\n",
        "svc.fit(X_train, y_train)\n",
        "acc_unscaled = svc.score(X_test, y_test)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "svc.fit(X_train_scaled, y_train)\n",
        "acc_scaled = svc.score(X_test_scaled, y_test)\n",
        "print(\"Unscaled Accuracy:\", acc_unscaled)\n",
        "print(\"Scaled Accuracy:\", acc_scaled)\n",
        "\n",
        "#10) Write a Python program to train a Gaussian Naïve Bayes model and compare the predictions before and after Laplace Smoothing:\n",
        "# Note: GaussianNB doesn't support Laplace smoothing (it's for MultinomialNB).\n"
      ]
    }
  ]
}

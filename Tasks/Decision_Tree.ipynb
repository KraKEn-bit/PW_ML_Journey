{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### 1) **What is a Decision Tree, and how does it work?**\n",
        "\n",
        "A **Decision Tree** is a supervised machine learning algorithm used for both classification and regression tasks. It splits the dataset into subsets based on feature values, using a tree-like structure. Each internal node represents a feature (or attribute), each branch represents a decision rule, and each leaf node represents an output label or value.\n",
        "\n",
        "**How it works:**\n",
        "\n",
        "* Start at the root node.\n",
        "* At each step, choose the best feature to split the data based on an impurity measure (like Gini or Entropy).\n",
        "* Repeat the process recursively for each child node until stopping conditions are met (e.g., pure leaves, max depth).\n",
        "\n",
        "---\n",
        "\n",
        "### 2) **What are impurity measures in Decision Trees?**\n",
        "\n",
        "Impurity measures quantify the **degree of disorder or impurity** in a set of examples. The goal of a decision tree is to reduce this impurity at each step.\n",
        "\n",
        "Common impurity measures:\n",
        "\n",
        "* **Gini Impurity**\n",
        "* **Entropy (Information Gain)**\n",
        "* **Variance (for regression tasks)**\n",
        "\n",
        "---\n",
        "\n",
        "### 3) **What is the mathematical formula for Gini Impurity?**\n",
        "\n",
        "The **Gini Impurity** of a node is:\n",
        "\n",
        "$$\n",
        "Gini = 1 - \\sum_{i=1}^{C} p_i^2\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $C$ is the number of classes.\n",
        "* $p_i$ is the probability (frequency) of class $i$ at that node.\n",
        "\n",
        "---\n",
        "\n",
        "### 4) **What is the mathematical formula for Entropy?**\n",
        "\n",
        "The **Entropy** of a node is:\n",
        "\n",
        "$$\n",
        "Entropy = -\\sum_{i=1}^{C} p_i \\log_2(p_i)\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $p_i$ is the proportion of class $i$ in the node.\n",
        "* $C$ is the total number of classes.\n",
        "\n",
        "---\n",
        "\n",
        "### 5) **What is Information Gain, and how is it used in Decision Trees?**\n",
        "\n",
        "**Information Gain** measures the **reduction in entropy or impurity** after splitting a node.\n",
        "\n",
        "$$\n",
        "IG = Entropy(parent) - \\sum_{j=1}^{k} \\frac{N_j}{N} \\cdot Entropy(child_j)\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $N$ is the total number of samples in the parent node.\n",
        "* $N_j$ is the number of samples in the child node $j$.\n",
        "* The tree chooses the split that gives the highest Information Gain.\n",
        "\n",
        "---\n",
        "\n",
        "### 6) **What is the difference between Gini Impurity and Entropy?**\n",
        "\n",
        "| Feature  | Gini Impurity                     | Entropy                         |\n",
        "| -------- | --------------------------------- | ------------------------------- |\n",
        "| Formula  | $1 - \\sum p_i^2$                  | $-\\sum p_i \\log_2(p_i)$         |\n",
        "| Speed    | Slightly faster to compute        | Slightly slower                 |\n",
        "| Behavior | Similar for binary classification | Entropy penalizes impurity more |\n",
        "| Usage    | Default in **CART**               | Used in **ID3** algorithm       |\n",
        "\n",
        "---\n",
        "\n",
        "### 7) **What is the mathematical explanation behind Decision Trees?**\n",
        "\n",
        "Mathematically, decision trees perform:\n",
        "\n",
        "* **Recursive binary partitioning**: At each node, select a feature and a threshold to split the data to minimize impurity.\n",
        "* **Objective Function**:\n",
        "\n",
        "  $$\n",
        "  \\text{Choose feature and threshold that minimizes: } \\sum \\frac{N_j}{N} \\cdot \\text{Impurity}(child_j)\n",
        "  $$\n",
        "* This is a greedy algorithm aiming to optimize each split locally.\n",
        "\n",
        "---\n",
        "\n",
        "### 8) **What is Pre-Pruning in Decision Trees?**\n",
        "\n",
        "**Pre-Pruning** stops the tree growth **before** it becomes too complex. Conditions include:\n",
        "\n",
        "* Maximum depth reached\n",
        "* Minimum samples at node\n",
        "* Impurity reduction is below a threshold\n",
        "* Node becomes \"pure\"\n",
        "\n",
        "**Goal**: Prevent overfitting early by stopping unnecessary splits.\n",
        "\n",
        "---\n",
        "\n",
        "### 9) **What is Post-Pruning in Decision Trees?**\n",
        "\n",
        "**Post-Pruning** allows the tree to grow fully and **then removes** branches that have little impact on prediction accuracy.\n",
        "\n",
        "Techniques:\n",
        "\n",
        "* Cost Complexity Pruning (CCP)\n",
        "* Reduced error pruning using validation set\n",
        "\n",
        "**Goal**: Simplify the model while retaining high accuracy.\n",
        "\n",
        "---\n",
        "\n",
        "### 10) **What is the difference between Pre-Pruning and Post-Pruning?**\n",
        "\n",
        "| Feature      | Pre-Pruning                       | Post-Pruning                       |\n",
        "| ------------ | --------------------------------- | ---------------------------------- |\n",
        "| When Applied | During tree building              | After full tree is built           |\n",
        "| Method       | Prevents splitting at a point     | Removes branches from full tree    |\n",
        "| Risk         | May stop too early (underfitting) | More flexible and accurate pruning |\n",
        "\n",
        "---\n",
        "\n",
        "### 11) **What is a Decision Tree Regressor?**\n",
        "\n",
        "A **Decision Tree Regressor** is a type of decision tree used for **predicting continuous values** rather than class labels.\n",
        "\n",
        "**Splitting Criterion**: Minimizes **Mean Squared Error (MSE)** or **Mean Absolute Error (MAE)**.\n",
        "\n",
        "---\n",
        "\n",
        "### 12) **What are the advantages and disadvantages of Decision Trees?**\n",
        "\n",
        "**Advantages:**\n",
        "\n",
        "* Easy to interpret and visualize\n",
        "* Handles both numerical and categorical data\n",
        "* No need for feature scaling\n",
        "* Can capture non-linear relationships\n",
        "\n",
        "**Disadvantages:**\n",
        "\n",
        "* Prone to overfitting\n",
        "* Sensitive to small data changes\n",
        "* Can create biased trees if one class dominates\n",
        "\n",
        "---\n",
        "\n",
        "### 13) **How does a Decision Tree handle missing values?**\n",
        "\n",
        "Decision Trees can handle missing values using:\n",
        "\n",
        "* **Surrogate splits**: Use an alternative feature that gives a similar split.\n",
        "* **Imputation**: Replace missing values with mean/median/mode before training.\n",
        "* **Skipping**: Skip samples with missing values during training (less preferred).\n",
        "\n",
        "---\n",
        "\n",
        "### 14) **How does a Decision Tree handle categorical features?**\n",
        "\n",
        "* Splits can be done on **specific categories**: e.g., “Color = Red?”\n",
        "* Internally, the algorithm treats each category as a discrete value.\n",
        "* For features with many categories, grouping might be done to optimize splits.\n",
        "\n",
        "---\n",
        "\n",
        "### 15) **What are some real-world applications of Decision Trees?**\n",
        "\n",
        "* **Medical Diagnosis**: Predict disease based on symptoms\n",
        "* **Loan Approval**: Decide if a loan should be granted\n",
        "* **Customer Churn Prediction**: Identify customers likely to leave\n",
        "* **Fraud Detection**: Detect abnormal transaction patterns\n",
        "* **Credit Scoring**: Assess creditworthiness of a customer\n",
        "* **Recommendation Systems**: Suggest products based on preferences\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "amyJM2HmY6GE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Xu8WEckY2vi"
      },
      "outputs": [],
      "source": [
        "# 1) Train a Decision Tree Classifier on the Iris dataset and print model accuracy\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, mean_squared_error\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor, export_graphviz\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import graphviz\n",
        "import numpy as np\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "clf = DecisionTreeClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "print(\"\\n#1) Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "# 2) Train using Gini Impurity and print feature importances\n",
        "clf_gini = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf_gini.fit(X_train, y_train)\n",
        "print(\"\\n#2) Feature importances (Gini):\", clf_gini.feature_importances_)\n",
        "\n",
        "# 3) Train using Entropy and print model accuracy\n",
        "clf_entropy = DecisionTreeClassifier(criterion='entropy', random_state=42)\n",
        "clf_entropy.fit(X_train, y_train)\n",
        "y_pred_entropy = clf_entropy.predict(X_test)\n",
        "print(\"\\n#3) Accuracy (Entropy):\", accuracy_score(y_test, y_pred_entropy))\n",
        "\n",
        "# 4) Decision Tree Regressor on housing dataset with MSE\n",
        "housing = fetch_california_housing()\n",
        "Xh_train, Xh_test, yh_train, yh_test = train_test_split(housing.data, housing.target, test_size=0.3, random_state=42)\n",
        "reg = DecisionTreeRegressor()\n",
        "reg.fit(Xh_train, yh_train)\n",
        "y_pred_housing = reg.predict(Xh_test)\n",
        "print(\"\\n#4) MSE (Housing):\", mean_squared_error(yh_test, y_pred_housing))\n",
        "\n",
        "# 5) Visualize the Decision Tree using Graphviz\n",
        "dot_data = export_graphviz(clf, out_file=None, feature_names=iris.feature_names, class_names=iris.target_names, filled=True)\n",
        "graph = graphviz.Source(dot_data)\n",
        "print(\"\\n#5) Tree Visualization saved as 'iris_tree.png'\")\n",
        "graph.render(\"iris_tree\", format='png', cleanup=True)\n",
        "\n",
        "# 6) Max depth of 3 vs full depth tree\n",
        "clf_full = DecisionTreeClassifier(random_state=42)\n",
        "clf_limited = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "clf_full.fit(X_train, y_train)\n",
        "clf_limited.fit(X_train, y_train)\n",
        "print(\"\\n#6) Full Depth Accuracy:\", accuracy_score(y_test, clf_full.predict(X_test)))\n",
        "print(\"#6) Max Depth=3 Accuracy:\", accuracy_score(y_test, clf_limited.predict(X_test)))\n",
        "\n",
        "# 7) min_samples_split=5 vs default\n",
        "clf_default = DecisionTreeClassifier(random_state=42)\n",
        "clf_split5 = DecisionTreeClassifier(min_samples_split=5, random_state=42)\n",
        "clf_default.fit(X_train, y_train)\n",
        "clf_split5.fit(X_train, y_train)\n",
        "print(\"\\n#7) Default Tree Accuracy:\", accuracy_score(y_test, clf_default.predict(X_test)))\n",
        "print(\"#7) min_samples_split=5 Accuracy:\", accuracy_score(y_test, clf_split5.predict(X_test)))\n",
        "\n",
        "# 8) Apply feature scaling before training and compare\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "X_tr_scaled, X_te_scaled, y_tr_scaled, y_te_scaled = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
        "clf_scaled = DecisionTreeClassifier()\n",
        "clf_scaled.fit(X_tr_scaled, y_tr_scaled)\n",
        "print(\"\\n#8) Accuracy with Unscaled:\", accuracy_score(y_test, clf_default.predict(X_test)))\n",
        "print(\"#8) Accuracy with Scaled:\", accuracy_score(y_te_scaled, clf_scaled.predict(X_te_scaled)))\n",
        "\n",
        "# 9) Use One-vs-Rest for multiclass classification\n",
        "ovr = OneVsRestClassifier(DecisionTreeClassifier(random_state=42))\n",
        "ovr.fit(X_train, y_train)\n",
        "print(\"\\n#9) Accuracy with OvR:\", accuracy_score(y_test, ovr.predict(X_test)))\n",
        "\n",
        "# 10) Display feature importance scores\n",
        "print(\"\\n#10) Feature Importances (Default Tree):\")\n",
        "for name, importance in zip(iris.feature_names, clf.feature_importances_):\n",
        "    print(f\"{name}: {importance:.4f}\")\n",
        "\n",
        "# 11) Regressor with max_depth=5 vs unrestricted\n",
        "reg_default = DecisionTreeRegressor(random_state=42)\n",
        "reg_depth5 = DecisionTreeRegressor(max_depth=5, random_state=42)\n",
        "reg_default.fit(Xh_train, yh_train)\n",
        "reg_depth5.fit(Xh_train, yh_train)\n",
        "print(\"\\n#11) MSE (Default):\", mean_squared_error(yh_test, reg_default.predict(Xh_test)))\n",
        "print(\"#11) MSE (Max Depth=5):\", mean_squared_error(yh_test, reg_depth5.predict(Xh_test)))\n",
        "\n",
        "# 12) Cost Complexity Pruning and visualize effect\n",
        "path = clf.cost_complexity_pruning_path(X_train, y_train)\n",
        "ccp_alphas = path.ccp_alphas[:-1]\n",
        "clfs = [DecisionTreeClassifier(random_state=0, ccp_alpha=alpha).fit(X_train, y_train) for alpha in ccp_alphas]\n",
        "acc = [accuracy_score(y_test, clf.predict(X_test)) for clf in clfs]\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(ccp_alphas, acc, marker='o')\n",
        "plt.title(\"#12) Accuracy vs CCP Alpha\")\n",
        "plt.xlabel(\"Alpha\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# 13) Evaluate using Precision, Recall, and F1-Score\n",
        "y_pred_pr = clf.predict(X_test)\n",
        "print(\"\\n#13) Precision:\", precision_score(y_test, y_pred_pr, average='macro'))\n",
        "print(\"#13) Recall:\", recall_score(y_test, y_pred_pr, average='macro'))\n",
        "print(\"#13) F1-Score:\", f1_score(y_test, y_pred_pr, average='macro'))\n",
        "\n",
        "# 14) Visualize confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred_pr)\n",
        "sns.heatmap(cm, annot=True, cmap='Blues', fmt='d', xticklabels=iris.target_names, yticklabels=iris.target_names)\n",
        "plt.title(\"#14) Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.show()\n",
        "\n",
        "# 15) GridSearchCV for max_depth and min_samples_split\n",
        "params = {\n",
        "    'max_depth': [2, 3, 4, 5, None],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "grid = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid=params, cv=5)\n",
        "grid.fit(X_train, y_train)\n",
        "print(\"\\n#15) Best Params:\", grid.best_params_)\n",
        "print(\"#15) Best Accuracy:\", grid.best_score_)\n"
      ]
    }
  ]
}
